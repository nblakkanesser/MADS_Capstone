{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "from environment import env\n",
    "config = env.env()\n",
    "\n",
    "OPENAI_API_KEY = config['gpt_api_key']\n",
    "nps_api_key = config['nps_api_key']\n",
    "\n",
    "from langchain_openai import OpenAI \n",
    "import chainlit as cl\n",
    "from langchain.chains import LLMChain, APIChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "from langchain.chains import APIChain\n",
    "\n",
    "from tfidf_model import TfidfClassifier\n",
    "from get_context import *\n",
    "import pickle\n",
    "\n",
    "model = pickle.load(open('../03_nps_models/tfidf_model.pkl','rb'))\n",
    "\n",
    "\n",
    "assistant_template = \"\"\"\n",
    "You are an national parks service assistant chatbot named \"Park Pal\". Your expertise is \n",
    "exclusively in providing information directly from the national parks service API. This includes queries related to amenities, events, alerts, park fees, park locations, and park descriptions. You do not provide information outside of this \n",
    "scope. If a question is not about an API endpoint listed previously, respond with, \"I specialize only in queries related to amenities, events, alerts, park fees, park locations, and park descriptions.\" \n",
    "Question: {question} \n",
    "Answer:\"\"\"\n",
    "\n",
    "api_url_template = \"\"\"\n",
    "Given the following API Documentation for the national parks service API: {api_docs}\n",
    "Your task is to interpret the data returned from the API to answer \n",
    "the user's question, ensuring the \n",
    "answer includes only necessary information.\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "api_url_prompt = PromptTemplate(input_variables=['api_docs', 'question'],\n",
    "                                template=api_url_template)\n",
    "\n",
    "api_response_template = \"\"\"\"\n",
    "With the API Documentation for the national parks service API: {api_docs} \n",
    "and the specific user question: {question} in mind,\n",
    "and given the variable api_url: {api_url} for querying,\n",
    "here is the response from the national parks service's API: {api_response}. \n",
    "Please provide a summary that directly addresses the user's question, \n",
    "omitting technical details like response format, and \n",
    "focusing on delivering the answer with clarity and conciseness, \n",
    "as if the national parks service itself is providing this information.\n",
    "Summary:\n",
    "\"\"\"\n",
    "api_response_prompt = PromptTemplate(input_variables=['api_docs', \n",
    "                                                      'question', \n",
    "                                                      'api_url',\n",
    "                                                      'api_response'],\n",
    "                                     template=api_response_template)\n",
    "\n",
    "\n",
    "@cl.on_chat_start\n",
    "def setup_multiple_chains():\n",
    "    llm = OpenAI(model='gpt-3.5-turbo-instruct',\n",
    "             temperature=0,\n",
    "             openai_api_key = OPENAI_API_KEY)\n",
    "    conversation_memory = ConversationBufferMemory(memory_key=\"chat_history\",\n",
    "                                                   max_len=200,\n",
    "                                                   return_messages=True,\n",
    "                                                   )\n",
    "    llm_chain = LLMChain(llm=llm, prompt=assistant_template, memory=conversation_memory)\n",
    "    cl.user_session.set(\"llm_chain\", llm_chain)\n",
    "\n",
    "    api_chain = APIChain.from_llm_and_api_docs(\n",
    "        llm=llm,\n",
    "        api_docs=nps_api_key,\n",
    "        api_url_prompt=api_url_prompt,\n",
    "        api_response_prompt=api_response_prompt,\n",
    "        verbose=True\n",
    "    )\n",
    "    cl.user_session.set(\"api_chain\", api_chain)\n",
    "\n",
    "@cl.on_message\n",
    "async def handle_message(message: cl.Message):\n",
    "    user_message = message.content.lower()\n",
    "    api_url,endpoint = api_call(model.get_params(user_message))\n",
    "    llm_chain = cl.user_session.get(\"llm_chain\")\n",
    "    api_chain = cl.user_session.get(\"api_chain\")\n",
    "\n",
    "    if endpoint in ['alerts']:\n",
    "        # If endpoint is specified endpoint use api_chain\n",
    "        response = await api_chain.acall(user_message, \n",
    "                                         callbacks=[cl.AsyncLangchainCallbackHandler()])\n",
    "    else:\n",
    "        # Default to llm_chain for handling general queries\n",
    "        response = await llm_chain.acall(user_message, \n",
    "                                         callbacks=[cl.AsyncLangchainCallbackHandler()])\n",
    "    response_key = \"output\" if \"output\" in response else \"text\"\n",
    "    await cl.Message(response.get(response_key, \"\")).send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chatbot.py\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "from environment import NPS_API\n",
    "from environment import gpt_model_functions\n",
    "from environment import env\n",
    "config = env.env()\n",
    "\n",
    "OPENAI_API_KEY = config['gpt_api_key']\n",
    "\n",
    "from langchain_openai import OpenAI \n",
    "import chainlit as cl\n",
    "from langchain.chains import LLMChain, APIChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "from langchain.chains import APIChain\n",
    "\n",
    "\n",
    "assistant_template = \"\"\"\n",
    "You are an national parks service assistant chatbot named \"Park Pal\". Your expertise is \n",
    "exclusively in providing information directly from the national parks service API. This includes queries related to amenities, events, alerts, park fees, park locations, and park descriptions. You do not provide information outside of this \n",
    "scope. If a question is not about an API endpoint listed previously, respond with, \"I specialize only in queries related to amenities, events, alerts, park fees, park locations, and park descriptions.\" \n",
    "Question: {question} \n",
    "Answer:\"\"\"\n",
    "\n",
    "api_url_template = \"\"\"\n",
    "Given the following API Documentation for the national parks service API: {api_docs}\n",
    "Your task is to interpret the data returned from the API to answer \n",
    "the user's question, ensuring the \n",
    "answer includes only necessary information.\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "api_url_prompt = PromptTemplate(input_variables=['api_docs', 'question'],\n",
    "                                template=api_url_template)\n",
    "\n",
    "api_response_template = \"\"\"\"\n",
    "With the API Documentation for the national parks service API: {api_docs} \n",
    "and the specific user question: {question} in mind,\n",
    "and given the variable api_url: {api_url} for querying,\n",
    "here is the response from the national parks service's API: {api_response}. \n",
    "Please provide a summary that directly addresses the user's question, \n",
    "omitting technical details like response format, and \n",
    "focusing on delivering the answer with clarity and conciseness, \n",
    "as if the national parks service itself is providing this information.\n",
    "Summary:\n",
    "\"\"\"\n",
    "api_response_prompt = PromptTemplate(input_variables=['api_docs', \n",
    "                                                      'question', \n",
    "                                                      'api_url',\n",
    "                                                      'api_response'],\n",
    "                                     template=api_response_template)\n",
    "\n",
    "\n",
    "@cl.on_chat_start\n",
    "def setup_multiple_chains():\n",
    "    llm = OpenAI(model='gpt-3.5-turbo-instruct',\n",
    "             temperature=0,\n",
    "             openai_api_key = OPENAI_API_KEY)\n",
    "    conversation_memory = ConversationBufferMemory(memory_key=\"chat_history\",\n",
    "                                                   max_len=200,\n",
    "                                                   return_messages=True,\n",
    "                                                   )\n",
    "    llm_chain = LLMChain(llm=llm, prompt=assistant_template, memory=conversation_memory)\n",
    "    cl.user_session.set(\"llm_chain\", llm_chain)\n",
    "\n",
    "    api_chain = APIChain.from_llm_and_api_docs(\n",
    "        llm=llm,\n",
    "        api_docs=NPS_API.api_docs(),\n",
    "        api_url_prompt=api_url_prompt,\n",
    "        api_response_prompt=api_response_prompt,\n",
    "        verbose=True\n",
    "    )\n",
    "    cl.user_session.set(\"api_chain\", api_chain)\n",
    "\n",
    "@cl.on_message\n",
    "async def handle_message(message: cl.Message):\n",
    "    user_message = message.content.lower()\n",
    "    api_url,endpoint = gpt_model_functions.create_url(user_message)\n",
    "    llm_chain = cl.user_session.get(\"llm_chain\")\n",
    "    api_chain = cl.user_session.get(\"api_chain\")\n",
    "\n",
    "    if endpoint in ['alerts']:\n",
    "        # If endpoint is specified endpoint use api_chain\n",
    "        response = await api_chain.acall(user_message, \n",
    "                                         callbacks=[cl.AsyncLangchainCallbackHandler()])\n",
    "    else:\n",
    "        # Default to llm_chain for handling general queries\n",
    "        response = await llm_chain.acall(user_message, \n",
    "                                         callbacks=[cl.AsyncLangchainCallbackHandler()])\n",
    "    response_key = \"output\" if \"output\" in response else \"text\"\n",
    "    await cl.Message(response.get(response_key, \"\")).send()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-23 13:12:32 - Your app is available at http://localhost:8000\n",
      "2024-07-23 13:12:35 - Translated markdown file for en-US not found. Defaulting to chainlit.md.\n",
      "2024-07-23 13:12:36 - You need to configure at least one of on_chat_start, on_message or on_audio_chunk callback\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchainlit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchatbot.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-w\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--port\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m8000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:1201\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:1264\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:2051\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2050\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 2051\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:2009\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   2008\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2009\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mwaitpid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid, wait_flags)\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   2012\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   2013\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   2014\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "subprocess.run([sys.executable, \"-m\", \"chainlit\", \"run\", \"chatbot.py\", \"-w\", \"--port\", \"8000\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
