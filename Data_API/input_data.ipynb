{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "api_key = '5TjgNMFCh7h44T09HbQnbGhU8as11D0FDdjfJhgV'\n",
    "api_base_url = 'https://developer.nps.gov/api/v1/'\n",
    "\n",
    "from helper_functions import *\n",
    "state_model, state_vectorizer = state_code_model.trained_model()\n",
    "park_model, park_vectorizer = park_code_model.trained_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parks(params):\n",
    "    \"\"\"\n",
    "    Use to find a list of all park names, codes, states, addresses and descriptions from the NPS parks endpoint.\n",
    "    Can also be used to find specific park information.\n",
    "\n",
    "    \"\"\"\n",
    "    parks = []\n",
    "    limit = 50  # Number of results per page, maximum allowed by NPS API\n",
    "    start = 0   # Initial starting point for pagination\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            'api_key': api_key,\n",
    "            'limit': limit,\n",
    "            'start': start\n",
    "        }\n",
    "        \n",
    "        response = requests.get(f\"{api_base_url}parks\", params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        parks.extend([\n",
    "            {\n",
    "                'fullName': park['fullName'],\n",
    "                'parkCode': park['parkCode'],\n",
    "                'state': park['states'],\n",
    "                'addresses': park.get('addresses', []),\n",
    "                'description': park['description']\n",
    "            } for park in data['data']\n",
    "        ])\n",
    "        \n",
    "        # Move to the next page\n",
    "        start += limit\n",
    "        \n",
    "        # Break the loop if all parks have been retrieved\n",
    "        if int(start) >= int(data['total']):\n",
    "            break\n",
    "    \n",
    "    return parks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parks_in_state(params):\n",
    "    \"\"\"\n",
    "    Retrieve a list of parks in a specified state.\n",
    "    \n",
    "    state_code: The code of the state (e.g., 'CA' for California)\n",
    "    api_key: Personal API key to use in request\n",
    "    \"\"\"\n",
    "    parks_in_state = []\n",
    "    \n",
    "    response = requests.get(f\"{api_base_url}parks\", params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    for park in data['data']:\n",
    "        if params['stateCode'] in park['states'].split(','):\n",
    "            parks_in_state.append(park['fullName'])\n",
    "    \n",
    "    return parks_in_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic(endpoint, params):\n",
    "    \"\"\"\n",
    "    Use to get all data from endpoint without specific processing\n",
    "\n",
    "    endpoint: The API endpoint to call\n",
    "    params: The param dict to pass through the API call\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    limit = 50  # Number of results per page, maximum allowed by NPS API\n",
    "    start = 0   # Initial starting point for pagination\n",
    "    \n",
    "    while True:\n",
    "        params['limit'] = limit\n",
    "        params['start'] =  start\n",
    "        \n",
    "        request = requests.get(f\"{api_base_url}{endpoint}\", params=params)\n",
    "        request_data = request.json()\n",
    "\n",
    "        for record in request_data['data']:\n",
    "            responses.extend([record])\n",
    "        \n",
    "        # Move to the next page\n",
    "        start += limit\n",
    "        \n",
    "        # Break the loop if all responses have been retrieved\n",
    "        if int(start) >= int(request_data['total']):\n",
    "            break\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(api_key, entities, entityCode, endpoint, intent, queries, response_call=0):\n",
    "    \"\"\"\n",
    "    Creates synthetic data in the necessary format for a specified API call.\n",
    "\n",
    "    api_key: Personal API key to use in request.\n",
    "    entities: List of items to loop through such as State, Parks, Amentities.\n",
    "    endpoint: the NPS API endpoint to call such as /activities or /parks.\n",
    "    intent: General label for queries in a particular group. \n",
    "            For example, the questions \"Tell me about {park}\" and \"I want to know more about {park}\" could both be categorized with the label \"GetParkInfo\".\n",
    "    queries: A list of queries you would like to associate with a given set of API calls.\n",
    "    entityCode: The entity code to be used in the API call. e.g. stateCode & parkCode\n",
    "        *This will likely need to be updated to have more dynamic functionality.\n",
    "    response_call: Pass through the function name that should be used to intiate the API call. The results will be recorded in the response column of the dataset in list format.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for entity in entities:\n",
    "        for query in queries:\n",
    "            # Create API parameters \n",
    "            if entityCode == \"parkCode\":\n",
    "                # Park name needs to be converted to park code\n",
    "                params = {'api_key': api_key,\n",
    "                        entityCode: park_code_model.map_park_code(query.format(entity=entity), park_model, park_vectorizer)\n",
    "                }\n",
    "            # if entityCode == \"stateCode\":\n",
    "            #     # State name needs to be converted to state code\n",
    "            #     params = {'api_key': api_key,\n",
    "            #             entityCode: state_code_model.map_state_code(query.format(entity=entity), state_model, state_vectorizer)\n",
    "            #     }\n",
    "                \n",
    "            # Set response function to use\n",
    "            if response_call == 0:\n",
    "                response = \"\"\n",
    "            elif intent == \"ParkInfo\":\n",
    "                response = get_parks(params)\n",
    "            elif intent == \"ParksInState\":\n",
    "                response = get_parks_in_state(params)\n",
    "            else: \n",
    "                response = get_basic(endpoint, params)\n",
    "\n",
    "            dataset.append({\n",
    "                \"query\": query.format(entity=entity),\n",
    "                \"intent\": intent,\n",
    "                \"api_call\": {\n",
    "                    \"endpoint\": endpoint,\n",
    "                    \"parkCode\": params[entityCode]\n",
    "                },\n",
    "                \"response\":response\n",
    "            })\n",
    "\n",
    "    synthetic_queries = pd.json_normalize(dataset)\n",
    "    return synthetic_queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activity_queries(raw_queries, activities):\n",
    "    queries = []\n",
    "    for query in raw_queries:\n",
    "        for activity in activities:\n",
    "            adjusted = query.format(activity=activity)+\"{entity}\"\n",
    "            queries.append(adjusted)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categories = ['activities', 'activities/parks', 'alerts', 'amenities','amenities/parksvisitorcenters',\n",
    "              'amenities/parksplaces', 'articles', 'campgrounds', 'events', 'feespasses', \n",
    "              'lessonplans', 'multimedia/audio', 'multimedia/galleries', 'newsreleases',\n",
    "              'parkinglots', 'parks', 'places', 'people', 'thingstodo', \n",
    "              'topics', 'topics/parks', 'tours', 'visitorcenters', ]\n",
    "\n",
    "\n",
    "# list of parks\n",
    "parks_df = pd.DataFrame(get_parks({'api_key': api_key}))\n",
    "parks = parks_df['fullName'].tolist()\n",
    "park_codes = parks_df['parkCode'].tolist()\n",
    "park_lookup = dict(zip(parks, park_codes))\n",
    "park_roots = nps_parks_root.nps_parks_root()\n",
    "# Parks combined is the combination of two lists: The full park names and estimated park name abbreviations that users might use (i.e. Acadia National Park vs Acadia)\n",
    "parks_combined = park_roots+parks\n",
    "\n",
    "dist_states = [\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \n",
    "                \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \n",
    "                \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \n",
    "                \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \n",
    "                \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \n",
    "                \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \n",
    "                \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \n",
    "                \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \n",
    "                \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \n",
    "                \"West Virginia\", \"Wisconsin\", \"Wyoming\"]\n",
    "\n",
    "activities_raw = [\n",
    "    'Arts and Culture','Astronomy','Biking','Boating','Camping','Climbing','Fishing','Food','Guided Tours',\n",
    "    'Hiking','Horse Trekking','Ice Skating','Junior Ranger Program','Paddling','Park Film','Shopping','Skiing','Snow Play','Snowmobiling',\n",
    "    'Snowshoeing','Swimming','Wildlife Watching'\n",
    "    ]\n",
    "\n",
    "activities = [\n",
    "    'Arts and Culture','Astronomy','Bike','Boat','Camp','Climb','Fish','Eat','Take Guided Tours', \n",
    "    'Hike','Horse Trek','Ice Skate','Junior Ranger Program','Paddle Boarding','Watch a Film','Shop','Ski','Play in Snow','Snowmobile',\n",
    "    'Snowshoe','Swim','Wildlife Watch'\n",
    "    ]\n",
    "activities_dict = dict(zip(activities, activities_raw))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Tell me about {entity}\",\"What is the full name of {entity}?\",\"What is the address of {entity}?\",\"Which state is {entity} located in?\",\"Give me a description of {entity}\",\n",
    "    \"Where is {entity} located?\",\"Can you provide the full name of {entity}?\",\"What is the location of {entity}?\",\"Tell me the address of {entity}\",\n",
    "    \"In which state can I find {entity}?\",\"Describe {entity} to me\"\n",
    "]\n",
    "ParkInfo = get_info(api_key, entities = parks_combined, entityCode = \"parkCode\", endpoint = \"parks\", intent = \"ParkInfo\", queries = queries, response_call = 0)\n",
    "\n",
    "queries = [\n",
    "    \"Which parks are in {entity}?\",\"What parks can be found in {entity}?\",\"List the parks located in {entity}\",\"Are there any national parks in {entity}?\",\n",
    "    \"What national parks are in {entity}?\",\"Can you tell me the parks in {entity}?\",\"Give me a list of parks in {entity}\",\"Which national parks are located in {entity}?\",\n",
    "    \"What parks are available in {entity}?\",\"Tell me the parks that are in {entity}\"\n",
    "]\n",
    "#ParksInState = get_info(api_key, entities = dist_states, entityCode = \"stateCode\", endpoint = \"parks\", intent = \"ParksInState\", queries = queries, response_call = 0)\n",
    "\n",
    "queries = [\n",
    "    \"What activities can I do at {entity}?\",\"Tell me about the activities available at {entity}\",\"What can I do at {entity}?\",\"List the activities at {entity}\",\n",
    "    \"What recreational activities are offered at {entity}?\",\"What outdoor activities can I enjoy at {entity}?\",\"What kind of activities are there at {entity}?\",\n",
    "    \"What are the top activities at {entity}?\",\"What fun things can I do at {entity}?\",\"What activities are recommended at {entity}?\"\n",
    "]\n",
    "#raw_queries =  [\"What activities are there to do at \", \"Can I {activity} in \", \"Is {activity} available at \", \"What {activity} activities are available in \"]\n",
    "#queries = activity_queries(raw_queries, activities)\n",
    "ParkActivities = get_info(api_key, entities = parks_combined, entityCode = \"parkCode\", endpoint = \"activities\", intent = \"ParkActivities\", queries = queries, response_call = 0)\n",
    "\n",
    "queries = [\n",
    "    \"What alerts are active at {entity} currently?\",\"Are there any current alerts at {entity}?\",\"Tell me the active alerts at {entity}\",\"What are the current alerts for {entity}?\",\n",
    "    \"Which alerts are active in {entity} right now?\",\"List the active alerts at {entity}\",\"Are there any alerts at {entity}?\",\"What are the present alerts for {entity}?\",\n",
    "    \"Can you provide the active alerts for {entity}?\",\"What are the ongoing alerts at {entity}?\"\n",
    "]\n",
    "ParkAlerts = get_info(api_key, entities = parks_combined, entityCode = \"parkCode\", endpoint = \"alerts\", intent = \"ParkAlerts\", queries = queries, response_call = 0)\n",
    "\n",
    "queries = [\n",
    "    \"What amenities exist at {entity}?\",\"Tell me about the amenities at {entity}\",\"What facilities are available at {entity}?\",\"List the amenities at {entity}\",\n",
    "    \"What kind of amenities can I find at {entity}?\",\"What services and facilities does {entity} offer?\",\"What amenities should I expect at {entity}?\",\n",
    "    \"What conveniences are available at {entity}?\",\"What amenities does {entity} have?\",\"What facilities are provided at {entity}?\"\n",
    "]\n",
    "ParkAmenities = get_info(api_key, entities = parks_combined, entityCode = \"parkCode\", endpoint = \"amenities\", intent = \"ParkAmenities\", queries = queries, response_call = 0)\n",
    "\n",
    "queries = [\"What events are happening at {entity}?\",\"Tell me about the events at {entity}\",\"What upcoming events are scheduled at {entity}?\",\n",
    "    \"Are there any events at {entity}?\",\"What kind of events are held at {entity}?\",\"What events can I attend at {entity}?\",\n",
    "    \"Are there any special events at {entity}?\",\"What events are planned at {entity}?\",\"What events are currently happening at {entity}?\",\"What activities and events are there at {entity}?\"\n",
    "]\n",
    "ParkEvents = get_info(api_key, entities = parks_combined, entityCode = \"parkCode\", endpoint = \"events\", intent = \"ParkEvents\", queries = queries, response_call = 0)\n",
    "\n",
    "queries = [\n",
    "    \"How much does it cost to get into {entity}?\",\"What is the entry fee for {entity}?\", \"Tell me about the entrance fees for {entity}\",\n",
    "    \"What are the admission fees for {entity}?\",\"Are there any fees to visit {entity}?\",\"What is the cost of admission to {entity}?\",\n",
    "    \"How expensive is it to visit {entity}?\",\"What are the ticket prices for {entity}?\",\"Do I need to pay to enter {entity}?\",\"Are there any charges to access {entity}?\"\n",
    "]\n",
    "ParkFees = get_info(api_key, entities = parks_combined, entityCode = \"parkCode\", endpoint = \"feespasses\", intent = \"ParkFees\", queries = queries, response_call = 0)\n",
    "\n",
    "queries =  [\n",
    "    \"Where can I park at {entity}?\",\"Tell me about parking options at {entity}\",\"What are the parking facilities like at {entity}?\",\"Where should I park when visiting {entity}?\",\n",
    "    \"What parking areas are available at {entity}?\",\"Is there parking available at {entity}?\",\"How is parking managed at {entity}?\",\"Can I find parking near {entity}?\",\n",
    "    \"Are there designated parking lots at {entity}?\",\"What are the parking arrangements at {entity}?\"\n",
    "]\n",
    "ParkParkingLots = get_info(api_key, entities = parks_combined, entityCode = \"parkCode\", endpoint = \"parkinglots\", intent = \"ParkParkingLots\", queries = queries, response_call = 0)\n",
    "\n",
    "queries = [\"What things can I do at {entity}?\",\"Tell me about attractions at {entity}\",\"What are the attractions at {entity}?\",\n",
    "    \"What are the main attractions of {entity}?\",\"What are the highlights of {entity}?\",\"What can I see and do at {entity}?\",\n",
    "    \"What are the recreational opportunities at {entity}?\",\"What are the popular things to do at {entity}?\",\n",
    "    \"What experiences are available at {entity}?\",\"What is there to do {entity}?\"]\n",
    "ParkThingsToDo = get_info(api_key, entities = parks_combined, entityCode = \"parkCode\", endpoint = \"thingstodo\", intent = \"ParkThingsToDo\", queries = queries, response_call = 0)\n",
    "\n",
    "queries = [\"What tours can I take at {entity}?\",\"Tell me about guided tours at {entity}\",\"Are there any guided tours available at {entity}?\",\"What guided experiences are offered at {entity}?\",\n",
    "    \"Can I join any tours at {entity}?\",\"What kind of guided tours are available at {entity}?\",\"Are there ranger-led tours at {entity}?\",\n",
    "    \"What are the tour options at {entity}?\",\"How can I book a tour at {entity}?\",\"Are there any special tours or programs at {entity}?\"]\n",
    "ParkTours = get_info(api_key, entities = parks_combined, entityCode = \"parkCode\", endpoint = \"tours\", intent = \"ParkTours\", queries = queries, response_call = 0)\n",
    "\n",
    "queries = [\"Where are the visitor centers located at {entity}?\",\"Tell me about visitor centers at {entity}\",\"What visitor centers can I find at {entity}?\",\n",
    "    \"Are there any visitor centers at {entity}?\",\"Where can I find information centers at {entity}?\",\"What are the visitor facilities like at {entity}?\",\"Can you guide me to the visitor centers at {entity}?\",\n",
    "    \"How many visitor centers are there at {entity}?\",\"What services do the visitor centers offer at {entity}?\",\"Are the visitor centers at {entity} open to the public?\"]\n",
    "ParkVisitorCenters = get_info(api_key, entities = parks_combined, entityCode = \"parkCode\", endpoint = \"visitorcenters\", intent = \"ParkVisitorCenters\", queries = queries, response_call = 0)\n",
    "\n",
    "synthetic_queries_df = pd.concat([ParkInfo, ParkActivities, ParkAlerts, ParkAmenities, ParkEvents, ParkFees, ParkParkingLots, ParkThingsToDo, ParkTours, ParkVisitorCenters], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "could not allocate 246939648 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17160\\4135904502.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Fit the pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    465\u001b[0m                     \u001b[0mn_samples_bootstrap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_samples_bootstrap\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m                 )\n\u001b[1;32m--> 467\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m             )\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1863\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1865\u001b[0m         \u001b[1;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1792\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"balanced\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    940\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m             \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m         )\n\u001b[0;32m    944\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    418\u001b[0m             )\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.DepthFirstTreeBuilder.build\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.DepthFirstTreeBuilder.build\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.Tree._add_node\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_tree.pyx\u001b[0m in \u001b[0;36msklearn.tree._tree.Tree._resize_c\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32msklearn\\tree\\_utils.pyx\u001b[0m in \u001b[0;36msklearn.tree._utils.safe_realloc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: could not allocate 246939648 bytes"
     ]
    }
   ],
   "source": [
    "# Create query and label data\n",
    "X = synthetic_queries_df['query']\n",
    "y = synthetic_queries_df[['api_call.endpoint', 'api_call.parkCode']]\n",
    "\n",
    "# Split training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing and modeling pipeline\n",
    "text_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', text_vectorizer),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_transformer, 'query')\n",
    "    ])\n",
    "\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query = [\"What activities can I do at Yellowstone?\"]\n",
    "predicted = model.predict(new_query)\n",
    "\n",
    "print(\"Predicted endpoint:\", predicted[0][0])\n",
    "print(\"Predicted parkCode:\", predicted[1][0])\n",
    "print(\"Predicted stateCode:\", predicted[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '09DF0950-D319-4557-A57E-04CD2F63FF42', 'name': 'Arts and Culture'},\n",
       " {'id': '13A57703-BB1A-41A2-94B8-53B692EB7238', 'name': 'Astronomy'},\n",
       " {'id': '7CE6E935-F839-4FEC-A63E-052B1DEF39D2', 'name': 'Biking'},\n",
       " {'id': '071BA73C-1D3C-46D4-A53C-00D5602F7F0E', 'name': 'Boating'},\n",
       " {'id': 'A59947B7-3376-49B4-AD02-C0423E08C5F7', 'name': 'Camping'},\n",
       " {'id': 'B12FAAB9-713F-4B38-83E4-A273F5A43C77', 'name': 'Climbing'},\n",
       " {'id': 'C11D3746-5063-4BD0-B245-7178D1AD866C', 'name': 'Compass and GPS'},\n",
       " {'id': 'AE42B46C-E4B7-4889-A122-08FE180371AE', 'name': 'Fishing'},\n",
       " {'id': '1DFACD97-1B9C-4F5A-80F2-05593604799E', 'name': 'Food'},\n",
       " {'id': 'B33DC9B6-0B7D-4322-BAD7-A13A34C584A3', 'name': 'Guided Tours'},\n",
       " {'id': '42FD78B9-2B90-4AA9-BC43-F10E9FEA8B5A', 'name': 'Hands-On'},\n",
       " {'id': 'BFF8C027-7C8F-480B-A5F8-CD8CE490BFBA', 'name': 'Hiking'},\n",
       " {'id': '0307955A-B65C-4CE4-A780-EB36BAAADF0B', 'name': 'Horse Trekking'},\n",
       " {'id': '5FF5B286-E9C3-430E-B612-3380D8138600', 'name': 'Ice Skating'},\n",
       " {'id': 'DF4A35E0-7983-4A3E-BC47-F37B872B0F25',\n",
       "  'name': 'Junior Ranger Program'},\n",
       " {'id': '4D224BCA-C127-408B-AC75-A51563C42411', 'name': 'Paddling'},\n",
       " {'id': '0C0D142F-06B5-4BE1-8B44-491B90F93DEB', 'name': 'Park Film'},\n",
       " {'id': '24380E3F-AD9D-4E38-BF13-C8EEB21893E7', 'name': 'Shopping'},\n",
       " {'id': 'F9B1D433-6B86-4804-AED7-B50A519A3B7C', 'name': 'Skiing'},\n",
       " {'id': 'C38B3C62-1BBF-4EA1-A1A2-35DE21B74C17', 'name': 'Snow Play'},\n",
       " {'id': '7C912B83-1B1B-4807-9B66-97C12211E48E', 'name': 'Snowmobiling'},\n",
       " {'id': '01D717BC-18BB-4FE4-95BA-6B13AD702038', 'name': 'Snowshoeing'},\n",
       " {'id': '587BB2D3-EC35-41B2-B3F7-A39E2B088AEE', 'name': 'Swimming'},\n",
       " {'id': '0B685688-3405-4E2A-ABBA-E3069492EC50', 'name': 'Wildlife Watching'}]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint = 'activities'\n",
    "park = 'acad'\n",
    "params = {'api_key': api_key,\n",
    "        'parkCode': park\n",
    "                }\n",
    "test = get_basic(endpoint, params)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activities_list(params):\n",
    "    \"\"\"\n",
    "    Use to get a list of activities available at a specific park based on park code\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the endpoint for the activities query\n",
    "    activities_endpoint = f\"{api_base_url}activities/parks\"\n",
    "    \n",
    "    # get activities information\n",
    "    response = requests.get(activities_endpoint, params=params)\n",
    "    activities_data = response.json()\n",
    "    \n",
    "    # Extract the activities\n",
    "    activities = []\n",
    "    for activity in activities_data['data']:\n",
    "        if any(park['parkCode'] == params['parkCode'] for park in activity['parks']):\n",
    "            activities.append(activity['name'])\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_activities = list(set(activities))\n",
    "    \n",
    "    return unique_activities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
