{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nps_parks_root'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29624\\2830821858.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhelper_functions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'5TjgNMFCh7h44T09HbQnbGhU8as11D0FDdjfJhgV'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Courtney Gibson\\Documents\\02 - Education\\MADS 2021\\Capstone\\MADS_Capstone\\Data_API\\helper_functions\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhelper_functions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstate_code_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhelper_functions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnps_parks_root\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhelper_functions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpark_code_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Courtney Gibson\\Documents\\02 - Education\\MADS 2021\\Capstone\\MADS_Capstone\\Data_API\\helper_functions\\park_code_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhelper_functions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnps_parks_root\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnps_parks_root\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nps_parks_root'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,f1_score,roc_auc_score\n",
    "\n",
    "from helper_functions import *\n",
    "\n",
    "api_key = '5TjgNMFCh7h44T09HbQnbGhU8as11D0FDdjfJhgV'\n",
    "api_base_url = 'https://developer.nps.gov/api/v1/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parks(params):\n",
    "    \"\"\"\n",
    "    Use to find a list of all park names, codes, states, addresses and descriptions from the NPS parks endpoint.\n",
    "    Can also be used to find specific park information.\n",
    "    \n",
    "    api_key: Personal API key to use in request\n",
    "    \"\"\"\n",
    "    parks = []\n",
    "    limit = 50  # Number of results per page, maximum allowed by NPS API\n",
    "    start = 0   # Initial starting point for pagination\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            'api_key': api_key,\n",
    "            'limit': limit,\n",
    "            'start': start\n",
    "        }\n",
    "        \n",
    "        response = requests.get(f\"{api_base_url}parks\", params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        parks.extend([\n",
    "            {\n",
    "                'fullName': park['fullName'],\n",
    "                'parkCode': park['parkCode'],\n",
    "                'state': park['states'],\n",
    "                'addresses': park.get('addresses', []),\n",
    "                'description': park['description']\n",
    "            } for park in data['data']\n",
    "        ])\n",
    "        \n",
    "        # Move to the next page\n",
    "        start += limit\n",
    "        \n",
    "        # Break the loop if all parks have been retrieved\n",
    "        if int(start) >= int(data['total']):\n",
    "            break\n",
    "    \n",
    "    return parks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_states(raw_queries,park_codes,parks,park_abbreviations):\n",
    "    \"\"\"\n",
    "    Creates synthetic query data that will be used as training data for a model that identifies the state being asked about in a query.\n",
    "\n",
    "    raw_queries: List of queries to loop through and create data for.\n",
    "    park_codes: Park codes associated with the park name.\n",
    "    parks: List of parks to create the queries for.\n",
    "    park_abbreviations: List of park abbreviations to create the queries for.\n",
    "    \"\"\"\n",
    "    queries = []\n",
    "    query_park_name = []\n",
    "    query_park_code = []\n",
    "    for park_name, park_code, park_abbreviation in zip(parks, park_codes, park_abbreviations):\n",
    "        for query in raw_queries:\n",
    "            output = query.format(entity=park_name)\n",
    "            queries.append(output)\n",
    "            query_park_name.append(park_name)\n",
    "            query_park_code.append(park_code)\n",
    "\n",
    "            output = query.format(entity=park_abbreviation)\n",
    "            queries.append(output)\n",
    "            query_park_name.append(park_name)\n",
    "            query_park_code.append(park_code)\n",
    "        \n",
    "    data = {\n",
    "    'query': queries,\n",
    "    'parks': query_park_name,\n",
    "    'park_codes': query_park_code\n",
    "    }\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(synthetic_park_data):\n",
    "    # Vectorize the text data\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(synthetic_park_data['query'])\n",
    "    y = synthetic_park_data['park_codes']\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a logistic regression model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_model():\n",
    "    synthetic_park_data = generate_synthetic_states(raw_queries,park_codes,parks,park_roots)\n",
    "    model, vectorizer = train_model(synthetic_park_data)\n",
    "    return model, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_park_code(user_input):\n",
    "    \"\"\"\n",
    "    Map user input to the correct state code using the trained model.\n",
    "\n",
    "    user_input: The query provided by the user.\n",
    "    model: Trained classification model.\n",
    "    vectorizer: Fitted vectorizer for text processing.\n",
    "    \"\"\"\n",
    "    model, vectorizer = trained_model()\n",
    "\n",
    "    # Transform the user input\n",
    "    user_input_vectorized = vectorizer.transform([user_input])\n",
    "    \n",
    "    # Predict the state code\n",
    "    predicted_park_code = model.predict(user_input_vectorized)[0]\n",
    "    \n",
    "    return predicted_park_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_df = pd.DataFrame(get_parks({'api_key': api_key}))\n",
    "parks = parks_df['fullName'].tolist()\n",
    "park_codes = parks_df['parkCode'].tolist()\n",
    "park_lookup = dict(zip(parks, park_codes))\n",
    "park_roots = nps_parks_root.nps_parks_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_queries = ['What can I do in {entity}?',\n",
    "    'Where is {entity} located?',\n",
    "    'What are the best hiking trails in {entity}?',\n",
    "    'Are there camping facilities in {entity}?',\n",
    "    'How do I get to {entity}?',\n",
    "    'What wildlife can I see in {entity}?',\n",
    "    'Are there any guided tours available in {entity}?',\n",
    "    'What is the weather like in {entity}?',\n",
    "    'Can I bring my pet to {entity}?',\n",
    "    'Are there any entrance fees for {entity}?',\n",
    "    'What is the best time of year to visit {entity}?',\n",
    "    'Tell me about the history of {entity}.',\n",
    "    'Are there any special events happening at {entity}?',\n",
    "    'What are the must-see attractions in {entity}?',\n",
    "    'Can I swim in the lakes or rivers at {entity}?',\n",
    "    'What are the hours of operation for {entity}?',\n",
    "    'Is there lodging available inside {entity}?',\n",
    "    'What are the rules for fishing at {entity}?',\n",
    "    'Are there any restrictions on photography at {entity}?',\n",
    "    'Tell me about the geological features of {entity}.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Where is {entity} located?'\n",
    "entity = 'Great Sand Dunes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grsa'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_park_code(query.format(entity=entity))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
