{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement langchain-base (from versions: none)\n",
      "ERROR: No matching distribution found for langchain-base\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain-base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feespass\n",
    "query = 'How much are the enterance fees for Bryce Canyon?'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize LangChain\n",
    "lc = LangChain()\n",
    "\n",
    "# Define intents and their corresponding actions\n",
    "@lc.rule\n",
    "def handle_query_intent(query):\n",
    "    \"\"\"\n",
    "    Handle queries and return appropriate responses.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Call your API and process the query\n",
    "        output = api_call(query)\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "\n",
    "# Define a function to interact with the user\n",
    "def chat():\n",
    "    print(\"Welcome to the National Parks Service Chatbot!\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            print(\"Exiting the chatbot.\")\n",
    "            break\n",
    "        try:\n",
    "            response = lc.run(user_input)\n",
    "            print(\"Bot:\", response)\n",
    "        except LangChainError:\n",
    "            print(\"Bot: Sorry, I didn't understand that.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Bot: Sorry, something went wrong: {str(e)}\")\n",
    "\n",
    "# Entry point to start the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI \n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "import chainlit as cl\n",
    "from langchain.chains import APIChain\n",
    "import pandas as pd \n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "import pandas as pd\n",
    "from environment import env\n",
    "from environment import gpt_model_functions\n",
    "config = env.env()\n",
    "\n",
    "OPENAI_API_KEY = config['gpt_api_key']\n",
    "\n",
    "@tool\n",
    "def handle_query(query, model, client, max_tokens):\n",
    "    \"\"\"\n",
    "    Uses a fine tuned model to interpret query into necessary API results based on the model parameter. \n",
    "\n",
    "    query (str): A user defined query.\n",
    "    model (str): The job id of the fine tuned GPT model. Can be found on the GPT fine tuning dashboard under 'Job ID' on the associated model.\n",
    "    client (obj): Authorization through API key to GPT console.\n",
    "    max_tokens (int): Number of tokens to limit response to. (Parameter is a misnomer as the response will try to fit 50 tokens if max is set to 50)\n",
    "    * Function was created using OpenAI fine-tuning documentation\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "    prompt = f\"prompt: {query}\\n\"\n",
    "    response = client.completions.create(\n",
    "        model=model,  \n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    completion = response.choices[0].text\n",
    "    return completion\n",
    "\n",
    "@tool\n",
    "def get_params(query):\n",
    "    \"\"\"\n",
    "    Function to use finetuned model to find endpoint and parkcode for an API call on a specific query.\n",
    "\n",
    "    query (str): A user defined query.\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "    # Define input variables\n",
    "    client = OpenAI(api_key  = config['gpt_api_key'])\n",
    "    gpt_parkcode_model = config['gpt_parkcode_model']\n",
    "    gpt_endpoint_model = config['gpt_endpoint_model']\n",
    "    gpt_intent_model = config['gpt_intent_model']\n",
    "    intents = ['description','address','state','fullname','alerts','amenities','events','feespass']\n",
    "\n",
    "    # Load models from OpenAI\n",
    "    parkcode_model = client.fine_tuning.jobs.retrieve(gpt_parkcode_model).fine_tuned_model\n",
    "    endpoint_model = client.fine_tuning.jobs.retrieve(gpt_endpoint_model).fine_tuned_model\n",
    "    intent_model = client.fine_tuning.jobs.retrieve(gpt_intent_model).fine_tuned_model\n",
    "\n",
    "    # Predict endpoint and parkcode\n",
    "    max_tokens = 3\n",
    "    endpoint = handle_query(query,endpoint_model,client,max_tokens).replace('endpoint: ','')\n",
    "    max_tokens = 5\n",
    "    parkcode = handle_query(query,parkcode_model,client,max_tokens).replace('parkcode: ','')\n",
    "    max_tokens = 1\n",
    "    #The intents dont have a uniform number of tokens and this was my solution (I'd like to improve this logic)\n",
    "    if handle_query(query,intent_model,client,max_tokens) in intents:\n",
    "        intent = handle_query(query,intent_model,client,max_tokens)\n",
    "    elif handle_query(query,intent_model,client,2) in intents:\n",
    "        intent = handle_query(query,intent_model,client,2)\n",
    "    elif handle_query(query,intent_model,client,3) in intents:\n",
    "        intent = handle_query(query,intent_model,client,3)\n",
    "\n",
    "    return endpoint, parkcode, intent\n",
    "\n",
    "@tool\n",
    "def api_call(query):\n",
    "    \"\"\"\n",
    "    Use to get all data from endpoint without specific processing\n",
    "\n",
    "    endpoint: The API endpoint to call\n",
    "    params: The param dict to pass through the API call\n",
    "    * ChatGPT was used to create the pagination process for parsing the API data.\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "    api_base_url = 'https://developer.nps.gov/api/v1/'\n",
    "    endpoint, parkcode, intent = get_params(query)\n",
    "    responses = []\n",
    "    limit = 50  # Number of results per page, maximum allowed by NPS API\n",
    "    start = 0   # Initial starting point for pagination\n",
    "    \n",
    "    while True:\n",
    "        params = {'api_key': config['nps_api_key'],\n",
    "                  'parkCode': parkcode,\n",
    "                  'limit' : limit,\n",
    "                  'start' : start,\n",
    "                }\n",
    "        \n",
    "        if endpoint == 'fees':\n",
    "            endpoint = 'feespasses'\n",
    "        request = requests.get(f\"{api_base_url}{endpoint}\", params=params)\n",
    "        request_data = request.json()\n",
    "\n",
    "        # Limit park data to necessary fields\n",
    "        if endpoint == 'parks':\n",
    "            responses.extend([\n",
    "                {\n",
    "                    'fullName': park['fullName'],\n",
    "                    'parkCode': park['parkCode'],\n",
    "                    'state': park['states'],\n",
    "                    'addresses': park.get('addresses', []),\n",
    "                    'description': park['description']\n",
    "                } for park in request_data['data']\n",
    "            ])\n",
    "        else:\n",
    "            for record in request_data['data']:\n",
    "                responses.extend([record])\n",
    "        \n",
    "        # Move to the next page\n",
    "        start += limit\n",
    "        \n",
    "        # Break the loop if all responses have been retrieved\n",
    "        if int(start) >= int(request_data['total']):\n",
    "            break\n",
    "\n",
    "    #output = parse_endpoint(endpoint, parkcode, intent, responses)\n",
    "    output = pd.DataFrame(responses) \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 15:14:05 - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_params',\n",
       "  'args': {'query': 'Yosemite'},\n",
       "  'id': 'call_x6IHQ77xMgepd7TpOB91Z3tZ'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [handle_query, get_params]#, api_call\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",openai_api_key = OPENAI_API_KEY)\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools, tool_choice=\"get_params\")\n",
    "\n",
    "query = 'What is the parkcode for Yosemite?'\n",
    "out = llm_with_tools.invoke(query).tool_calls\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
