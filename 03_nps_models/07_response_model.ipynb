{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "import chainlit as cl\n",
    "from langchain.chains import APIChain\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "import pandas as pd\n",
    "from environment import env\n",
    "from environment import gpt_model_functions\n",
    "config = env.env()\n",
    "\n",
    "OPENAI_API_KEY = config['gpt_api_key']\n",
    "\n",
    "assistant_template = \"\"\"\n",
    "You are an national parks service assistant chatbot named \"Park Pal\". Your expertise is \n",
    "exclusively in providing information directly from the national parks service API. This includes queries related to amenities, events, alerts, park fees, park locations, and park descriptions. You do not provide information outside of this \n",
    "scope. If a question is not about an API endpoint listed previously, respond with, \"I specialize only in queries related to amenities, events, alerts, park fees, park locations, and park descriptions.\" \n",
    "Question: {question} \n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=assistant_template\n",
    ")\n",
    "\n",
    "@cl.on_chat_start\n",
    "def quey_llm():\n",
    "    llm = OpenAI(model='gpt-3.5-turbo-instruct',\n",
    "             temperature=0.7,\n",
    "             openai_api_key = OPENAI_API_KEY)\n",
    "    \n",
    "    conversation_memory = ConversationBufferMemory(memory_key=\"chat_history\",\n",
    "                                                   max_len=50,\n",
    "                                                   return_messages=True,\n",
    "                                                   )\n",
    "    llm_chain = LLMChain(llm=llm, \n",
    "                         prompt=prompt_template,\n",
    "                         memory=conversation_memory)\n",
    "    \n",
    "    cl.user_session.set(\"llm_chain\", llm_chain)\n",
    "\n",
    "@cl.on_message\n",
    "async def query_llm(message: cl.Message):\n",
    "    llm_chain = cl.user_session.get(\"llm_chain\")\n",
    "    \n",
    "    response = await llm_chain.acall(message.content, \n",
    "                                     callbacks=[\n",
    "                                         cl.AsyncLangchainCallbackHandler()])\n",
    "    \n",
    "    await cl.Message(response[\"text\"]).send()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
