{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-17 11:03:16 - Created default config file at /Users/lauralyns/Documents/MADS/SIADS 699 - Capstone/Capstone VS/MADS_Capstone/03_nps_models/.chainlit/config.toml\n",
      "2024-07-17 11:03:16 - Created default translation directory at /Users/lauralyns/Documents/MADS/SIADS 699 - Capstone/Capstone VS/MADS_Capstone/03_nps_models/.chainlit/translations\n",
      "2024-07-17 11:03:16 - Created default translation file at /Users/lauralyns/Documents/MADS/SIADS 699 - Capstone/Capstone VS/MADS_Capstone/03_nps_models/.chainlit/translations/en-US.json\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "from environment import env\n",
    "config = env.env()\n",
    "\n",
    "OPENAI_API_KEY = config['gpt_api_key']\n",
    "\n",
    "from langchain_openai import OpenAI \n",
    "import chainlit as cl\n",
    "from langchain.chains import LLMChain, APIChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "from langchain.chains import APIChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain = (\n",
    "    {}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "from environment import NPS_API\n",
    "from environment import gpt_model_functions\n",
    "from environment import env\n",
    "config = env.env()\n",
    "\n",
    "OPENAI_API_KEY = config['gpt_api_key']\n",
    "\n",
    "from langchain_openai import OpenAI \n",
    "import chainlit as cl\n",
    "from langchain.chains import LLMChain, APIChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "from langchain.chains import APIChain\n",
    "\n",
    "\n",
    "assistant_template = \"\"\"\n",
    "You are an national parks service assistant chatbot named \"Park Pal\". Your expertise is \n",
    "exclusively in providing information directly from the national parks service API. This includes queries related to amenities, events, alerts, park fees, park locations, and park descriptions. You do not provide information outside of this \n",
    "scope. If a question is not about an API endpoint listed previously, respond with, \"I specialize only in queries related to amenities, events, alerts, park fees, park locations, and park descriptions.\" \n",
    "Question: {question} \n",
    "Answer:\"\"\"\n",
    "\n",
    "api_url_template = \"\"\"\n",
    "Given the following API Documentation for the national parks service API: {api_docs}\n",
    "Your task is to interpret the data returned from the API to answer \n",
    "the user's question, ensuring the \n",
    "answer includes only necessary information.\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "api_url_prompt = PromptTemplate(input_variables=['api_docs', 'question'],\n",
    "                                template=api_url_template)\n",
    "\n",
    "api_response_template = \"\"\"\"\n",
    "With the API Documentation for the national parks service API: {api_docs} \n",
    "and the specific user question: {question} in mind,\n",
    "and given the variable api_url: {api_url} for querying,\n",
    "here is the response from the national parks service's API: {api_response}. \n",
    "Please provide a summary that directly addresses the user's question, \n",
    "omitting technical details like response format, and \n",
    "focusing on delivering the answer with clarity and conciseness, \n",
    "as if the national parks service itself is providing this information.\n",
    "Summary:\n",
    "\"\"\"\n",
    "api_response_prompt = PromptTemplate(input_variables=['api_docs', \n",
    "                                                      'question', \n",
    "                                                      'api_url',\n",
    "                                                      'api_response'],\n",
    "                                     template=api_response_template)\n",
    "\n",
    "\n",
    "@cl.on_chat_start\n",
    "def setup_multiple_chains():\n",
    "    llm = OpenAI(model='gpt-3.5-turbo-instruct',\n",
    "             temperature=0,\n",
    "             openai_api_key = OPENAI_API_KEY)\n",
    "    conversation_memory = ConversationBufferMemory(memory_key=\"chat_history\",\n",
    "                                                   max_len=200,\n",
    "                                                   return_messages=True,\n",
    "                                                   )\n",
    "    llm_chain = LLMChain(llm=llm, prompt=assistant_template, memory=conversation_memory)\n",
    "    cl.user_session.set(\"llm_chain\", llm_chain)\n",
    "\n",
    "    api_chain = APIChain.from_llm_and_api_docs(\n",
    "        llm=llm,\n",
    "        api_docs=NPS_API.api_docs(),\n",
    "        api_url_prompt=api_url_prompt,\n",
    "        api_response_prompt=api_response_prompt,\n",
    "        verbose=True\n",
    "    )\n",
    "    cl.user_session.set(\"api_chain\", api_chain)\n",
    "\n",
    "@cl.on_message\n",
    "async def handle_message(message: cl.Message):\n",
    "    user_message = message.content.lower()\n",
    "    api_url,endpoint = gpt_model_functions.create_url(user_message)\n",
    "    llm_chain = cl.user_session.get(\"llm_chain\")\n",
    "    api_chain = cl.user_session.get(\"api_chain\")\n",
    "\n",
    "    if endpoint in ['alerts']:\n",
    "        # If endpoint is specified endpoint use api_chain\n",
    "        response = await api_chain.acall(user_message, \n",
    "                                         callbacks=[cl.AsyncLangchainCallbackHandler()])\n",
    "    else:\n",
    "        # Default to llm_chain for handling general queries\n",
    "        response = await llm_chain.acall(user_message, \n",
    "                                         callbacks=[cl.AsyncLangchainCallbackHandler()])\n",
    "    response_key = \"output\" if \"output\" in response else \"text\"\n",
    "    await cl.Message(response.get(response_key, \"\")).send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chatbot.py\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "from environment import NPS_API\n",
    "from environment import gpt_model_functions\n",
    "from environment import env\n",
    "config = env.env()\n",
    "\n",
    "OPENAI_API_KEY = config['gpt_api_key']\n",
    "\n",
    "from langchain_openai import OpenAI \n",
    "import chainlit as cl\n",
    "from langchain.chains import LLMChain, APIChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "from langchain.chains import APIChain\n",
    "\n",
    "\n",
    "assistant_template = \"\"\"\n",
    "You are an national parks service assistant chatbot named \"Park Pal\". Your expertise is \n",
    "exclusively in providing information directly from the national parks service API. This includes queries related to amenities, events, alerts, park fees, park locations, and park descriptions. You do not provide information outside of this \n",
    "scope. If a question is not about an API endpoint listed previously, respond with, \"I specialize only in queries related to amenities, events, alerts, park fees, park locations, and park descriptions.\" \n",
    "Question: {question} \n",
    "Answer:\"\"\"\n",
    "\n",
    "api_url_template = \"\"\"\n",
    "Given the following API Documentation for the national parks service API: {api_docs}\n",
    "Your task is to interpret the data returned from the API to answer \n",
    "the user's question, ensuring the \n",
    "answer includes only necessary information.\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "api_url_prompt = PromptTemplate(input_variables=['api_docs', 'question'],\n",
    "                                template=api_url_template)\n",
    "\n",
    "api_response_template = \"\"\"\"\n",
    "With the API Documentation for the national parks service API: {api_docs} \n",
    "and the specific user question: {question} in mind,\n",
    "and given the variable api_url: {api_url} for querying,\n",
    "here is the response from the national parks service's API: {api_response}. \n",
    "Please provide a summary that directly addresses the user's question, \n",
    "omitting technical details like response format, and \n",
    "focusing on delivering the answer with clarity and conciseness, \n",
    "as if the national parks service itself is providing this information.\n",
    "Summary:\n",
    "\"\"\"\n",
    "api_response_prompt = PromptTemplate(input_variables=['api_docs', \n",
    "                                                      'question', \n",
    "                                                      'api_url',\n",
    "                                                      'api_response'],\n",
    "                                     template=api_response_template)\n",
    "\n",
    "\n",
    "@cl.on_chat_start\n",
    "def setup_multiple_chains():\n",
    "    llm = OpenAI(model='gpt-3.5-turbo-instruct',\n",
    "             temperature=0,\n",
    "             openai_api_key = OPENAI_API_KEY)\n",
    "    conversation_memory = ConversationBufferMemory(memory_key=\"chat_history\",\n",
    "                                                   max_len=200,\n",
    "                                                   return_messages=True,\n",
    "                                                   )\n",
    "    llm_chain = LLMChain(llm=llm, prompt=assistant_template, memory=conversation_memory)\n",
    "    cl.user_session.set(\"llm_chain\", llm_chain)\n",
    "\n",
    "    api_chain = APIChain.from_llm_and_api_docs(\n",
    "        llm=llm,\n",
    "        api_docs=NPS_API.api_docs(),\n",
    "        api_url_prompt=api_url_prompt,\n",
    "        api_response_prompt=api_response_prompt,\n",
    "        verbose=True\n",
    "    )\n",
    "    cl.user_session.set(\"api_chain\", api_chain)\n",
    "\n",
    "@cl.on_message\n",
    "async def handle_message(message: cl.Message):\n",
    "    user_message = message.content.lower()\n",
    "    api_url,endpoint = gpt_model_functions.create_url(user_message)\n",
    "    llm_chain = cl.user_session.get(\"llm_chain\")\n",
    "    api_chain = cl.user_session.get(\"api_chain\")\n",
    "\n",
    "    if endpoint in ['alerts']:\n",
    "        # If endpoint is specified endpoint use api_chain\n",
    "        response = await api_chain.acall(user_message, \n",
    "                                         callbacks=[cl.AsyncLangchainCallbackHandler()])\n",
    "    else:\n",
    "        # Default to llm_chain for handling general queries\n",
    "        response = await llm_chain.acall(user_message, \n",
    "                                         callbacks=[cl.AsyncLangchainCallbackHandler()])\n",
    "    response_key = \"output\" if \"output\" in response else \"text\"\n",
    "    await cl.Message(response.get(response_key, \"\")).send()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ran code in interactive window to get it to run\n",
    "import sys\n",
    "subprocess.run([sys.executable, \"-m\", \"chainlit\", \"run\", \"chatbot.py\", \"-w\", \"--port\", \"8000\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
